{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashk121/pyspark-interview/blob/main/PysparkQuestions_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 1\n"
      ],
      "metadata": {
        "id": "88TjORw2qqJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 1: Data Skew and Optimization\n",
        "\n",
        "You have two large dataframes:\n",
        "\n",
        "transactions_df: Contains millions of transaction records—each with a user_id, amount, and timestamp.\n",
        "\n",
        "users_df: Contains user attributes (user_id, country, email).\n",
        "\n",
        "You need to aggregate total amount per country, but when you join transactions_df with users_df using user_id, the job is very slow and fails due to data skew.\n",
        "\n",
        "Task:\n",
        "Write PySpark code to efficiently perform this aggregation while handling data skew. Explain your strategy—such as using broadcast joins, repartitioning, or adding salt keys."
      ],
      "metadata": {
        "id": "A-WX9YwohUUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "W3ZkMmHPhB6I"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "spark = SparkSession.builder.appName(\"DailyRevenue\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DvA3ewVhM-D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to answer by Data skewness handling approach"
      ],
      "metadata": {
        "id": "8zTlrnuNoRqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(1, \"2023-01-01 10:00:00\", \"100\"),(1, \"2023-01-02 10:05:00\", \"200\"),(2, \"2023-01-03 11:00:00\", \"300\"),\\\n",
        "        (2, \"2023-01-04 11:10:00\", \"400\"),(1, \"2023-02-10 10:15:00\", \"500\"),(3, \"2023-01-11 12:00:00\", \"600\"),\\\n",
        "        (3, \"2023-09-01 12:05:00\", \"700\"),(3, \"2023-01-12 12:10:00\", \"800\") ]\n",
        "columns = [\"user_id\", \"timestamp\", \"amount\"]\n",
        "transactions_df = spark.createDataFrame(data , columns)\n",
        "transactions_df.show()"
      ],
      "metadata": {
        "id": "WtjKTNFFhM41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9393bd-3ab9-4d91-e0cb-6dd9e4d8fb1e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+------+\n",
            "|user_id|          timestamp|amount|\n",
            "+-------+-------------------+------+\n",
            "|      1|2023-01-01 10:00:00|   100|\n",
            "|      1|2023-01-02 10:05:00|   200|\n",
            "|      2|2023-01-03 11:00:00|   300|\n",
            "|      2|2023-01-04 11:10:00|   400|\n",
            "|      1|2023-02-10 10:15:00|   500|\n",
            "|      3|2023-01-11 12:00:00|   600|\n",
            "|      3|2023-09-01 12:05:00|   700|\n",
            "|      3|2023-01-12 12:10:00|   800|\n",
            "+-------+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0a415bd",
        "outputId": "2edde584-09f2-4be2-f746-80b0e22879d1"
      },
      "source": [
        "users_data = [\n",
        "    (1, \"USA\", \"user1@example.com\"),\n",
        "    (2, \"Canada\", \"user2@example.com\"),\n",
        "    (3, \"USA\", \"user3@example.com\"),\n",
        "    (4, \"UK\", \"user4@example.com\"),\n",
        "    (5, \"Canada\", \"user5@example.com\")\n",
        "]\n",
        "users_columns = [\"user_id\", \"country\", \"email\"]\n",
        "users_df = spark.createDataFrame(users_data, users_columns)\n",
        "users_df.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----------------+\n",
            "|user_id|country|            email|\n",
            "+-------+-------+-----------------+\n",
            "|      1|    USA|user1@example.com|\n",
            "|      2| Canada|user2@example.com|\n",
            "|      3|    USA|user3@example.com|\n",
            "|      4|     UK|user4@example.com|\n",
            "|      5| Canada|user5@example.com|\n",
            "+-------+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df_salted = transactions_df.withColumn(\"user_id_salted\" , concat(col(\"user_id\") ,lit('_'),lit(10 * rand()).cast(\"int\")))\n",
        "transactions_df_salted = transactions_df_salted.repartition(col(\"user_id_salted\"))\n",
        "users_df = users_df.withColumn(\"salt\" , F.explode(F.array([F.lit(i) for i in range(10)])))\n",
        "users_df_salted = users_df.withColumn(\"user_id_salted\" , concat(col(\"user_id\") ,lit('_'),col(\"salt\")))"
      ],
      "metadata": {
        "id": "BELa0EvNqgsd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = transactions_df_salted.join(users_df_salted, on=users_df_salted.user_id_salted == transactions_df_salted.user_id_salted, how=\"inner\")\n",
        "merged_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Cn204GKwMyp",
        "outputId": "091fd8dd-80c2-4bb5-9beb-d7f62c1edceb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+------+--------------+-------+-------+-----------------+\n",
            "|user_id|          timestamp|amount|user_id_salted|user_id|country|            email|\n",
            "+-------+-------------------+------+--------------+-------+-------+-----------------+\n",
            "|      1|2023-01-01 10:00:00|   100|           1_1|      1|    USA|user1@example.com|\n",
            "|      1|2023-01-02 10:05:00|   200|           1_2|      1|    USA|user1@example.com|\n",
            "|      1|2023-02-10 10:15:00|   500|           1_0|      1|    USA|user1@example.com|\n",
            "|      2|2023-01-03 11:00:00|   300|           2_0|      2| Canada|user2@example.com|\n",
            "|      2|2023-01-04 11:10:00|   400|           2_7|      2| Canada|user2@example.com|\n",
            "|      3|2023-01-11 12:00:00|   600|           3_5|      3|    USA|user3@example.com|\n",
            "|      3|2023-09-01 12:05:00|   700|           3_7|      3|    USA|user3@example.com|\n",
            "|      3|2023-01-12 12:10:00|   800|           3_1|      3|    USA|user3@example.com|\n",
            "+-------+-------------------+------+--------------+-------+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using broadcast join"
      ],
      "metadata": {
        "id": "eCnUoKlmzFFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = transactions_df.join(broadcast(users_df), on=transactions_df.user_id == users_df.user_id, how=\"inner\")\n",
        "merged_df.show()"
      ],
      "metadata": {
        "id": "WKJYPuqKzE0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85re8EoYw-Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scenario 2\n"
      ],
      "metadata": {
        "id": "By2LOQlSqlEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario 2: Rolling Aggregation with Window Functions\n",
        "\n",
        "You have a DataFrame called web_events_df with columns: user_id, event_time, and page_category. Each user can have multiple web events across different time points.\n",
        "\n",
        "Task:\n",
        "Write PySpark code to calculate the rolling 7-day count of events for each user—i.e., for each event, show how many events the user performed in the previous 7 days (including the current event).\n",
        "Explain your approach using PySpark’s window functions."
      ],
      "metadata": {
        "id": "4lWqI3BhhNro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a DataFrame called web_events_df with columns: user_id, event_time, and page_category.\n",
        "# Each user can have multiple web events across different time points.\n",
        "data=[(1, \"2023-01-01 10:00:00\", \"Home\"),(1, \"2023-01-02 10:05:00\", \"Products\"),(2, \"2023-01-03 11:00:00\", \"Home\"),\\\n",
        "        (2, \"2023-01-04 11:10:00\", \"Contact\"),(1, \"2023-02-10 10:15:00\", \"Cart\"),(3, \"2023-01-11 12:00:00\", \"Home\"),\\\n",
        "        (3, \"2023-09-01 12:05:00\", \"Products\"),(3, \"2023-01-12 12:10:00\", \"Checkout\")]\n",
        "columns = [\"user_id\", \"event_time\", \"page_category\"]\n",
        "web_events_df=spark.createDataFrame(data, columns)\n",
        "web_events_df=web_events_df.withColumn(\"event_time\", to_timestamp(col(\"event_time\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "web_events_df.orderBy(\"user_id\",\"event_time\").show()\n",
        "#web_events_df.printSchema()"
      ],
      "metadata": {
        "id": "LFmI0a5RhMtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e4355d-cff5-4661-c286-b54979a85e1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------+\n",
            "|user_id|         event_time|page_category|\n",
            "+-------+-------------------+-------------+\n",
            "|      1|2023-01-01 10:00:00|         Home|\n",
            "|      1|2023-01-02 10:05:00|     Products|\n",
            "|      1|2023-02-10 10:15:00|         Cart|\n",
            "|      2|2023-01-03 11:00:00|         Home|\n",
            "|      2|2023-01-04 11:10:00|      Contact|\n",
            "|      3|2023-01-11 12:00:00|         Home|\n",
            "|      3|2023-01-12 12:10:00|     Checkout|\n",
            "|      3|2023-09-01 12:05:00|     Products|\n",
            "+-------+-------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "web_events_df.createOrReplaceTempView(\"vw_web_events_df\")"
      ],
      "metadata": {
        "id": "Iv0OZnUThXQM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_df=spark.sql(\"select user_id,event_time,page_category, count(page_category) \\\n",
        "over(partition by user_id order by event_time range between interval '7' day preceding and current row)as rolling_7_sum from vw_web_events_df\")\n",
        "agg_df.orderBy(\"user_id\").show()\n",
        "### Explanation: windows"
      ],
      "metadata": {
        "id": "eVtsj7rZhXMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab7e01d9-ef1f-4ec4-ac72-b925816b18d5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------+-------------+\n",
            "|user_id|         event_time|page_category|rolling_7_sum|\n",
            "+-------+-------------------+-------------+-------------+\n",
            "|      1|2023-01-01 10:00:00|         Home|            1|\n",
            "|      1|2023-01-02 10:05:00|     Products|            2|\n",
            "|      1|2023-02-10 10:15:00|         Cart|            1|\n",
            "|      2|2023-01-03 11:00:00|         Home|            1|\n",
            "|      2|2023-01-04 11:10:00|      Contact|            2|\n",
            "|      3|2023-01-11 12:00:00|         Home|            1|\n",
            "|      3|2023-01-12 12:10:00|     Checkout|            2|\n",
            "|      3|2023-09-01 12:05:00|     Products|            1|\n",
            "+-------+-------------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying to use Pyspark code for the same logic\n"
      ],
      "metadata": {
        "id": "MOGfvV3ODme7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "web_events_df = web_events_df.withColumn(\"event_ts\", F.unix_timestamp(\"event_time\"))\n",
        "window = Window.partitionBy(col(\"user_id\")).orderBy(\"event_ts\").rangeBetween(-7*86400, 0)\n",
        "ans_df = web_events_df.select(col(\"user_id\"), col(\"event_time\") , col(\"page_category\") , count(\"*\").over(window).alias(\"rolling_7_sum\"))"
      ],
      "metadata": {
        "id": "qq_69lQDDqki"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  ans_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01s1fqBMEppD",
        "outputId": "271fc6b8-58b5-4851-c7ae-cd51cd855a2b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-------------+-------------+\n",
            "|user_id|         event_time|page_category|rolling_7_sum|\n",
            "+-------+-------------------+-------------+-------------+\n",
            "|      1|2023-01-01 10:00:00|         Home|            1|\n",
            "|      1|2023-01-02 10:05:00|     Products|            2|\n",
            "|      1|2023-02-10 10:15:00|         Cart|            1|\n",
            "|      2|2023-01-03 11:00:00|         Home|            1|\n",
            "|      2|2023-01-04 11:10:00|      Contact|            2|\n",
            "|      3|2023-01-11 12:00:00|         Home|            1|\n",
            "|      3|2023-01-12 12:10:00|     Checkout|            2|\n",
            "|      3|2023-09-01 12:05:00|     Products|            1|\n",
            "+-------+-------------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}